{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "MODEL_NAME = \"nlp.model\"\n",
    "EPOCH = 10\n",
    "BATCHSIZE = 128\n",
    "LR = 0.0001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import contractions\n",
    "import requests\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, url_en, url_vi):\n",
    "        # function to preprocessing\n",
    "        self.__tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        self.__tokenizer_vi = lambda text: list(map(lambda word: re.sub('_', ' ', word), tokenize(text).split()))\n",
    "\n",
    "        self.__check_dict = { # bá»• xung\n",
    "            ' \\'s': '\\'s',\n",
    "            '& lt ;': '<',\n",
    "            '& gt ;': '>',\n",
    "            \"<[^<]+>\":'',\n",
    "            ' +': ' ',\n",
    "        }\n",
    "\n",
    "        #last run\n",
    "        self.__en_data = self.__load_data(url_en, 'en')\n",
    "        self.__vi_data = self.__load_data(url_vi, 'vi')\n",
    "\n",
    "    def __text_preprocessing(self, text: str, language: str = 'en'):\n",
    "        text = html.unescape(text)\n",
    "        for pattern, repl in self.__check_dict.items():\n",
    "            text = re.sub(pattern, repl, text)\n",
    "\n",
    "        if language == 'en':\n",
    "            text = contractions.fix(text)\n",
    "            return self.__tokenizer_en(text)\n",
    "\n",
    "        return self.__tokenizer_vi(text)\n",
    "\n",
    "    def __load_data(self, url, language: str):\n",
    "        return [self.__text_preprocessing(line, language) for line in requests.get(url).text.splitlines()]\n",
    "\n",
    "    @property\n",
    "    def vi(self):\n",
    "        return self.__vi_data\n",
    "\n",
    "    @property\n",
    "    def en(self):\n",
    "        return self.__en_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
    "\n",
    "train = DataLoader(url +'train.en',url +'train.vi')\n",
    "test = DataLoader(url + 'tst2013.en',url + 'tst2013.vi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, train_iter: Iterator, min_freq:int = 1):\n",
    "        self.specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "        self.__make_vocab(train_iter,min_freq)\n",
    "    def __yield_tokens(self, data):\n",
    "        for line in data:\n",
    "            yield line  \n",
    "\n",
    "    def __make_vocab(self, train_iter: Iterator, min_freq:int = 1):\n",
    "        self.__vocab = build_vocab_from_iterator(self.__yield_tokens(train_iter), min_freq, self.specials)\n",
    "        self.__vocab.set_default_index(0)\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.__vocab\n",
    "    \n",
    "    def lookup_indices(self, token_list: List[str]):\n",
    "        return [2,*self.vocab.lookup_indices(token_list),3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "Vi = Language(train.vi,3)\n",
    "En = Language(train.en,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23971"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(En.word2index.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1656, 7408, 733, 114, 7, 1782, 2551, 160, 158, 24, 12, 5]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [SOS_IDX,*En.vocab.lookup_indices(['Over', '15,000', 'scientists', 'go', 'to', 'San', 'Francisco', 'every', 'year', 'for', 'that', '.'])]\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab.vocab import Vocab\n",
    "def data_preprocessing(data:List[List[str]],vocab:Vocab):\n",
    "    rr = []\n",
    "    idx2word = vocab.get_itos()\n",
    "    for line in data:\n",
    "        tkl = ['<sos>']\n",
    "        for word in line:\n",
    "            tkl.append(idx2word[vocab[word]])\n",
    "        tkl.append('<eos>')\n",
    "        rr.append(tkl)\n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_prep = data_preprocessing(train.en, En.vocab)\n",
    "train_vi_prep = data_preprocessing(train.vi, Vi.vocab)\n",
    "test_en_prep = data_preprocessing(test.en, En.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133317\n",
      "133317\n",
      "1268\n"
     ]
    }
   ],
   "source": [
    "print(len(train_en_prep))\n",
    "print(len(train_vi_prep))\n",
    "print(len(test_en_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133317"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(train_en_prep, train_vi_prep))\n",
    "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
    "test_data = list(zip(test_en_prep, test.en, test.vi))\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def make_batch(data:List[Tuple], batchsize:int):\n",
    "    bb = []\n",
    "    ben = []\n",
    "    bvi = []\n",
    "    for en, vi in data: \n",
    "        ben.append(en)\n",
    "        bvi.append(vi)\n",
    "        if len(ben) >= batchsize:\n",
    "            bb.append((ben, bvi))\n",
    "            ben = []\n",
    "            bvi = []\n",
    "    if len(ben) > 0:\n",
    "        bb.append((ben, bvi))\n",
    "    return bb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_batch(train_data, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_batch(b):\n",
    "    maxlen = max([len(x) for x in b])\n",
    "    for tkl in b:\n",
    "        for i in range(maxlen - len(tkl)):\n",
    "            tkl.append('<pad>')\n",
    "\n",
    "def padding(bb):\n",
    "    for ben, bvi in bb:\n",
    "        padding_batch(ben)\n",
    "        padding_batch(bvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(data:List[List[str]], vocab: Vocab):\n",
    "    return [[vocab[word] for word in word_lst] for word_lst in data]\n",
    "\n",
    "train_data = [(text_pipeline(ben,En.vocab),text_pipeline(bvi,Vi.vocab)) for ben, bvi in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocablist_x, vocabidx_x, vocablist_y, vocabidx_y):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_embed_x = len(vocablist_x)\n",
    "        self.num_embed_y = len(vocablist_y)\n",
    "\n",
    "        self.encemb = torch.nn.Embedding(self.num_embed_x, 256, padding_idx = vocabidx_x['<pad>'])\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.enclstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
    "        \n",
    "        self.decemb = torch.nn.Embedding(self.num_embed_x, 256, padding_idx = vocabidx_y['<pad>'])\n",
    "        self.declstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
    "        self.decout = torch.nn.Linear(516, self.num_embed_y)\n",
    "  \n",
    "    def forward(self,x):\n",
    "        x, y = x[0], x[1]\n",
    "\n",
    "        e_x = self.dropout(self.encemb(x))\n",
    "        \n",
    "        outenc,(hidden,cell) = self.enclstm(e_x)\n",
    "\n",
    "        n_y=y.shape[0]\n",
    "        print('n_y: ',y.shape)\n",
    "        # outputs = torch.zeros(n_y,BATCHSIZE,self.num_embed_x).to(DEVICE)\n",
    "        loss = torch.tensor(0.,dtype=torch.float32).to(DEVICE)\n",
    "        for i in range(n_y-1):\n",
    "            input = y[i]\n",
    "            input = input.unsqueeze(0)\n",
    "            input = self.dropout(self.decemb(input))\n",
    "            outdec, (hidden,cell) = self.declstm(input,(hidden,cell))\n",
    "            output = self.decout(outdec.squeeze(0))\n",
    "            input = y[i+1]\n",
    "            loss += F.cross_entropy(output, y[i+1])\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self,x,vocablist_y,vocabidx_y):\n",
    "        e_x = self.dropout(self.encemb(x))\n",
    "        outenc,(hidden,cell)=self.enclstm(e_x)\n",
    "        \n",
    "        y = torch.tensor([vocabidx_y['<cls>']]).to(DEVICE)\n",
    "        pred=[]\n",
    "        for i in range(30):\n",
    "            input = y\n",
    "            input = input.unsqueeze(0)\n",
    "            input = self.dropout(self.decemb(input))\n",
    "            outdec,(hidden,cell)= self.declstm(input,(hidden,cell))\n",
    "            output = self.decout(outdec.squeeze(0))  \n",
    "            pred_id = output.squeeze().argmax().item()\n",
    "            if pred_id == vocabidx_y['<eos>']:\n",
    "                break\n",
    "            pred_y = vocablist_y[pred_id]\n",
    "            pred.append(pred_y)\n",
    "            y[0]=pred_id\n",
    "            input=y\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LMST():\n",
    "    model = LSTM(En.index2word, En.word2index, Vi.index2word, Vi.word2index).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR) \n",
    "    for epoch in range(EPOCH):\n",
    "        loss = 0\n",
    "        step = 0\n",
    "        for ben, bvi in train_data:\n",
    "            ben = torch.tensor(ben, dtype=torch.int64).transpose(0,1).to(DEVICE) \n",
    "            bvi = torch.tensor(bvi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            batchloss = model((ben, bvi))\n",
    "            batchloss.backward()\n",
    "            optimizer.step() \n",
    "            loss = loss + batchloss.item()\n",
    "            if step % 100 == 0:\n",
    "                print(\"step:\", step, \"batch loss:\", batchloss.item())\n",
    "            step += 1\n",
    "        print(\"epoch\", epoch, \": loss\", loss)\n",
    "    torch.save(model.state_dict(), MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LMST()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **requirement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyvi) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.1.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.8)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.63.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download vi\n",
    "# %pip install underthesea\n",
    "# %pip install contractions\n",
    "%pip install pyvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
    "\n",
    "languages = ['en', 'vi']\n",
    "\n",
    "train_data = {\n",
    "    'en': requests.get(url+'train.en').text.splitlines(),\n",
    "    'vi': requests.get(url+'train.vi').text.splitlines()\n",
    "}\n",
    "test = {\n",
    "    'en': requests.get(url+'tst2013.en').text.splitlines(),\n",
    "    'vi': requests.get(url+'tst2013.vi').text.splitlines()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "import html\n",
    "import contractions\n",
    "import re\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "tokenizer = {\n",
    "    'en': get_tokenizer('spacy', language='en_core_web_sm'),\n",
    "    'vi': lambda text : list(map(lambda word: re.sub('_', ' ',word),tokenize(text).split()))\n",
    "}\n",
    "\n",
    "def text_preprocessing(text,language): \n",
    "    if language == 'en':\n",
    "        return re.sub(' +', ' ', contractions.fix(html.unescape(text)))\n",
    "    else:\n",
    "        return re.sub(' +', ' ',html.unescape(text))\n",
    "\n",
    "def yield_tokens(train_data, language='en'):\n",
    "    for line in train_data[language]:\n",
    "        yield tokenizer[language](text_preprocessing(line,language))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "vocabs = {\n",
    "    'en': build_vocab_from_iterator(yield_tokens(train_data, 'en'), 3, specials),\n",
    "    'vi': build_vocab_from_iterator(yield_tokens(train_data, 'vi'), 3, specials)\n",
    "}\n",
    "\n",
    "for language in languages:\n",
    "    vocabs[language].set_default_index(UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size en: 23973\n",
      "vocab size vi: 17843\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size en:\", len(vocabs['en'].get_itos()))\n",
    "print(\"vocab size vi:\", len(vocabs['vi'].get_itos()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

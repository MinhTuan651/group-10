{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "MODEL_NAME = \"nlp.model\"\n",
    "EPOCH = 10\n",
    "BATCHSIZE = 128\n",
    "LR = 0.0001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import contractions\n",
    "import requests\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, url_en:str, url_vi:str):\n",
    "        # function to preprocessing\n",
    "        DataLoader.__tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        DataLoader.__tokenizer_vi = lambda text: list(map(lambda word: re.sub('_', ' ', word), tokenize(text).split()))\n",
    "\n",
    "        DataLoader.__check_dict = { # bổ xung\n",
    "            ' \\'s': '\\'s',\n",
    "            '& lt ;': '<',\n",
    "            '& gt ;': '>',\n",
    "            \"<[^<]+>\":'',\n",
    "            ' +': ' ',\n",
    "        }\n",
    "\n",
    "        #last run\n",
    "        self.__load_data(url_en, url_vi)\n",
    "\n",
    "    def __text_preprocessing(self, text: str, language: str = 'en'):\n",
    "        text = html.unescape(text)\n",
    "        for pattern, repl in DataLoader.__check_dict.items():\n",
    "            text = text.lower()\n",
    "            text = re.sub(pattern, repl, text)\n",
    "\n",
    "        if language == 'en':\n",
    "            text = contractions.fix(text)\n",
    "            return DataLoader.__tokenizer_en(text)\n",
    "\n",
    "        return DataLoader.__tokenizer_vi(text)\n",
    "\n",
    "    def __load_data(self, url_en:str, url_vi:str):\n",
    "        data_en = requests.get(url_en).text.strip().splitlines()\n",
    "        data_vi = requests.get(url_vi).text.strip().splitlines()\n",
    "        self.__en_data = []\n",
    "        self.__vi_data = []\n",
    "        for en,vi in zip(data_en,data_vi):\n",
    "            en = [\"<sos>\",*self.__text_preprocessing(en, 'en'), \"<eos>\"] \n",
    "            vi = [\"<sos>\",*self.__text_preprocessing(vi, 'vi'), \"<eos>\"] \n",
    "            if len(en) < 33 and len(vi) < 33:\n",
    "                self.__en_data.append(en)\n",
    "                self.__vi_data.append(vi)\n",
    "\n",
    "    @property\n",
    "    def vi(self):\n",
    "        return self.__vi_data\n",
    "\n",
    "    @property\n",
    "    def en(self):\n",
    "        return self.__en_data\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        '''return en_data, vi_data'''\n",
    "        return list(zip(self.__en_data,self.__vi_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
    "\n",
    "train = DataLoader(url +'train.en',url +'train.vi')\n",
    "val = DataLoader(url + 'tst2012.en',url + 'tst2012.vi')\n",
    "test = DataLoader(url + 'tst2013.en',url + 'tst2013.vi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: ['<sos>', 'rachel', 'pike', ':', 'the', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
      "vi: ['<sos>', 'khoa học', 'đằng', 'sau', 'một', 'tiêu đề', 'về', 'khí hậu', '<eos>']\n",
      "en: ['<sos>', 'i', ' ', 'would', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
      "vi: ['<sos>', 'tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to lớn', 'của', 'những', 'nỗ lực', 'khoa học', 'đã', 'góp phần', 'làm nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
      "en: ['<sos>', 'they', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
      "vi: ['<sos>', 'cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh vực', 'trong', 'ngành', 'khoa học', 'khí quyển', '.', '<eos>']\n",
      "en: ['<sos>', 'that', 'report', 'was', 'written', 'by', '620', 'scientists', 'from', '40', 'countries', '.', '<eos>']\n",
      "vi: ['<sos>', 'nghiên cứu', 'được', 'viết', 'bởi', '620', 'nhà', 'khoa học', 'từ', '40', 'quốc gia', 'khác', 'nhau', '.', '<eos>']\n",
      "en: ['<sos>', 'they', 'wrote', 'almost', 'a', 'thousand', 'pages', 'on', 'the', 'topic', '.', '<eos>']\n",
      "vi: ['<sos>', 'họ', 'viết', 'gần', '1000', 'trang', 'về', 'chủ đề', 'này', '.', '<eos>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(105184, 1319, 957)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for en,vi in train.data[:5]:\n",
    "    print(f\"en: {en}\")\n",
    "    print(f\"vi: {vi}\")\n",
    "    \n",
    "len(train.data),len(val.data),len(test.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import List\n",
    "class Language:\n",
    "    def __init__(self, train_iter: Iterator, min_freq:int = 1):\n",
    "        Language.specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "        self.__make_vocab(train_iter,min_freq)\n",
    "    def __yield_tokens(self, data):\n",
    "        for line in data:\n",
    "            yield line  \n",
    "\n",
    "    def __make_vocab(self, train_iter: Iterator, min_freq:int = 5):\n",
    "        self.__vocab = build_vocab_from_iterator(self.__yield_tokens(train_iter), min_freq, self.specials)\n",
    "        self.__vocab.set_default_index(0)\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.__vocab\n",
    "    \n",
    "    def sentence_to_vector(self, sent:List[str]):\n",
    "        return torch.tensor(self.__vocab.lookup_indices(sent),dtype = torch.int64)\n",
    "    \n",
    "    def vector_to_sentence(self, vector:List[int]):\n",
    "        return self.__vocab.lookup_tokens(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "Vi = Language(train.vi,3)\n",
    "En = Language(train.en,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12131"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vi.vocab.get_itos())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15931"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(En.vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = [En.sentence_to_vector(line) for line in train.en]\n",
    "train_vi = [Vi.sentence_to_vector(line) for line in train.vi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: ['<sos>', 'rachel', 'pike', ':', 'the', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
      "vi: ['<sos>', 'khoa học', 'đằng', 'sau', 'một', 'tiêu đề', 'về', 'khí hậu', '<eos>']\n",
      "en: ['<sos>', 'i', ' ', 'would', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
      "vi: ['<sos>', 'tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to lớn', 'của', 'những', 'nỗ lực', 'khoa học', 'đã', 'góp phần', 'làm nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
      "en: ['<sos>', 'they', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
      "vi: ['<sos>', 'cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh vực', 'trong', 'ngành', 'khoa học', 'khí quyển', '.', '<eos>']\n",
      "en: ['<sos>', 'that', 'report', 'was', 'written', 'by', '620', 'scientists', 'from', '40', 'countries', '.', '<eos>']\n",
      "vi: ['<sos>', 'nghiên cứu', 'được', 'viết', 'bởi', '620', 'nhà', 'khoa học', 'từ', '40', 'quốc gia', 'khác', 'nhau', '.', '<eos>']\n",
      "en: ['<sos>', 'they', 'wrote', 'almost', 'a', 'thousand', 'pages', 'on', 'the', 'topic', '.', '<eos>']\n",
      "vi: ['<sos>', 'họ', 'viết', 'gần', '1000', 'trang', 'về', 'chủ đề', 'này', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for en,vi in zip(train_en[:5],train_vi[:5]):\n",
    "    print(f\"en: {En.vector_to_sentence(en.tolist())}\")\n",
    "    print(f\"vi: {Vi.vector_to_sentence(vi.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "train_en = pad_sequence(train_en,batch_first= True,padding_value=UNK_IDX)\n",
    "train_vi = pad_sequence(train_vi,batch_first= True,padding_value=UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_en, train_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader as dl\n",
    "train_data = list(dl(train_data,batch_size=BATCHSIZE,shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=32):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
    "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

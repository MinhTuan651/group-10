{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "MODEL_NAME = \"nlp.model\"\n",
    "EPOCH = 10\n",
    "BATCHSIZE = 128\n",
    "LR = 0.0001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import contractions\n",
    "import requests\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, url_en, url_vi):\n",
    "        # function to preprocessing\n",
    "        self.__tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        self.__tokenizer_vi = lambda text: list(map(lambda word: re.sub('_', ' ', word), tokenize(text).split()))\n",
    "\n",
    "        self.__check_dict = { # bổ xung\n",
    "            ' \\'s': '\\'s',\n",
    "            '& lt ;': '<',\n",
    "            '& gt ;': '>',\n",
    "            \"<[^<]+>\":'',\n",
    "            ' +': ' ',\n",
    "        }\n",
    "\n",
    "        #last run\n",
    "        self.__en_data = self.__load_data(url_en, 'en')\n",
    "        self.__vi_data = self.__load_data(url_vi, 'vi')\n",
    "\n",
    "    def __text_preprocessing(self, text: str, language: str = 'en'):\n",
    "        text = html.unescape(text)\n",
    "        for pattern, repl in self.__check_dict.items():\n",
    "            text = re.sub(pattern, repl, text)\n",
    "\n",
    "        if language == 'en':\n",
    "            text = contractions.fix(text)\n",
    "            return self.__tokenizer_en(text)\n",
    "\n",
    "        return self.__tokenizer_vi(text)\n",
    "\n",
    "    def __load_data(self, url, language: str):\n",
    "        return [self.__text_preprocessing(line, language) for line in requests.get(url).text.splitlines()]\n",
    "\n",
    "    @property\n",
    "    def vi(self):\n",
    "        return self.__vi_data\n",
    "\n",
    "    @property\n",
    "    def en(self):\n",
    "        return self.__en_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
    "\n",
    "train = DataLoader(url +'train.en',url +'train.vi')\n",
    "test = DataLoader(url + 'tst2013.en',url + 'tst2013.vi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Over', '15,000', 'scientists', 'go', 'to', 'San', 'Francisco', 'every', 'year', 'for', 'that', '.']\n",
      "10 ['Mỗi', 'năm', ',', 'hơn', '15,000', 'nhà', 'khoa học', 'đến', 'San Francisco', 'để', 'tham dự', 'hội nghị', 'này', '.']\n",
      "11 ['And', 'every', 'one', 'of', 'those', 'scientists', 'is', 'in', 'a', 'research', 'group', ',', 'and', 'every', 'research', 'group', 'studies', 'a', 'wide', 'variety', 'of', 'topics', '.']\n",
      "11 ['Mỗi một', 'khoa học', 'gia', 'đều', 'thuộc', 'một', 'nhóm', 'nghiên cứu', ',', 'và', 'mỗi', 'nhóm', 'đều', 'nghiên cứu', 'rất', 'nhiều', 'đề tài', 'đa dạng', '.']\n",
      "12 ['For', 'us', 'at', 'Cambridge', ',', 'it', 'is', 'as', 'varied', 'as', 'the', 'El', 'Niño', 'oscillation', ',', 'which', 'affects', 'weather', 'and', 'climate', ',', 'to', 'the', 'assimilation', 'of', 'satellite', 'data', ',', 'to', 'emissions', 'from', 'crops', 'that', 'produce', 'biofuels', ',', 'which', 'is', 'what', 'I', 'happen', 'to', 'study', '.']\n",
      "12 ['Với', 'chúng tôi', ',', 'tại', 'Cambridge', ',', 'các', 'đề tài', 'thay đổi', 'từ', 'sự', 'dao động', 'của', 'El Niño', ',', 'vốn', 'có', 'tác động', 'đến', 'thời tiết', 'và', 'khí hậu', ',', 'sự', 'đồng hoá', 'thông tin', 'từ', 'vệ tinh', ',', 'khí thải', 'từ', 'những', 'cánh', 'đồng', 'nhiên liệu', 'sinh học', ',', 'tình cờ', 'lại', 'là', 'đề tài', 'tôi', 'nghiên cứu', '.']\n",
      "13 ['And', 'in', 'each', 'one', 'of', 'these', 'research', 'areas', ',', 'of', 'which', 'there', 'are', 'even', 'more', ',', 'there', 'are', 'PhD', 'students', ',', 'like', 'me', ',', 'and', 'we', 'study', 'incredibly', 'narrow', 'topics', ',', 'things', 'as', 'narrow', 'as', 'a', 'few', 'processes', 'or', 'a', 'few', 'molecules', '.']\n",
      "13 ['Mỗi', 'lĩnh vực', 'nghiên cứu', 'lại', 'chia', 'ra', 'những', 'lĩnh vực', 'nhỏ', 'hơn', ',', 'và', 'những', 'nghiên cứu sinh', 'có', 'bằng', 'tiến sĩ', ',', 'như', 'tôi', ',', 'phải', 'nghiên cứu', 'những', 'đề tài', 'vô cùng', 'cụ thể', ',', 'cụ thể', 'như', 'chỉ', 'vài', 'quy trình', 'hay', 'vài', 'phân tử', '.']\n",
      "14 ['And', 'one', 'of', 'the', 'molecules', 'I', 'study', 'is', 'called', 'isoprene', ',', 'which', 'is', 'here', '.', 'It', 'is', 'a', 'small', 'organic', 'molecule', '.', 'You', \"'\", 've', 'probably', 'never', 'heard', 'of', 'it', '.']\n",
      "14 ['Một', 'trong', 'số', 'những', 'phân tử', 'tôi', 'nghiên cứu', 'tên', 'là', 'isoprene', '.', 'Đây', '.', 'Nó', 'là', 'một', 'phân tử', 'hữu cơ', 'nhỏ', '.', 'Có thể', 'các', 'bạn', 'cũng', 'chưa', 'từng', 'nghe', 'tên', '.']\n",
      "15 ['The', 'weight', 'of', 'a', 'paper', 'clip', 'is', 'approximately', 'equal', 'to', '900', 'zeta', '-', 'illion', '--', '10', 'to', 'the', '21st', '--', 'molecules', 'of', 'isoprene', '.']\n",
      "15 ['Trọng lượng', 'của', 'một', 'chiếc', 'kẹp', 'giấy', 'vào khoảng', '900', 'zeta', '-', 'illion', '-', '-', '10', 'mũ', '21', '-', '-', 'phân tử', 'isoprene', '.']\n",
      "16 ['But', 'despite', 'its', 'very', 'small', 'weight', ',', 'enough', 'of', 'it', 'is', 'emitted', 'into', 'the', 'atmosphere', 'every', 'year', 'to', 'equal', 'the', 'weight', 'of', 'all', 'the', 'people', 'on', 'the', 'planet', '.']\n",
      "16 ['Dù', 'trọng lượng', 'phân tử', 'rất', 'nhỏ', ',', 'thế', 'nhưng', 'lượng', 'isoprene', 'được', 'thải', 'vào', 'khí quyển', 'hàng', 'năm', 'ngang ngửa', 'với', 'tổng', 'trọng lượng', 'của', 'dân số', 'toàn cầu', '.']\n",
      "17 ['It', 'is', 'a', 'huge', 'amount', 'of', 'stuff', '.', 'It', 'is', 'equal', 'to', 'the', 'weight', 'of', 'methane', '.']\n",
      "17 ['Đó', 'là', 'một', 'lượng', 'khí thải', 'khổng lồ', ',', 'bằng', 'tổng', 'trọng lượng', 'của', 'mêtan', '.']\n",
      "18 ['And', 'because', 'it', 'is', 'so', 'much', 'stuff', ',', 'it', 'is', 'really', 'important', 'for', 'the', 'atmospheric', 'system', '.']\n",
      "18 ['Chính', 'vì', 'lượng', 'khí thải', 'rất', 'lớn', ',', 'nó', 'có', 'ý nghĩa', 'quan trọng', 'với', 'hệ thống', 'khí quyển', '.']\n",
      "19 ['Because', 'it', 'is', 'important', 'to', 'the', 'atmospheric', 'system', ',', 'we', 'go', 'to', 'all', 'lengths', 'to', 'study', 'this', 'thing', '.']\n",
      "19 ['Chính', 'vì', 'nó', 'có', 'ý nghĩa', 'quan trọng', 'với', 'hệ thống', 'khí quyển', ',', 'giá', 'nào', 'chúng tôi', 'cũng', 'theo đuổi', 'nghiên cứu', 'này', 'đến', 'cùng', '.']\n",
      "133317\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    print(i,train.en[i])\n",
    "    print(i,train.vi[i])\n",
    "\n",
    "print(len(train.en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, train_iter: Iterator, min_freq:int = 1,specials: Optional[List[str]] = None, default_idx:int = 0):\n",
    "        self.__make_vocab(train_iter,min_freq,specials,default_idx)\n",
    "\n",
    "    def __yield_tokens(self, data):\n",
    "        for line in data:\n",
    "            yield line  \n",
    "\n",
    "    def __make_vocab(self, train_iter: Iterator, min_freq:int = 1,specials: Optional[List[str]] = None, default_idx:int = 0):\n",
    "        self.__vocab = build_vocab_from_iterator(self.__yield_tokens(train_iter), min_freq, specials)\n",
    "        self.__vocab.set_default_index(default_idx)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    @property\n",
    "    def word2index(self):\n",
    "        return self.__vocab.get_stoi()\n",
    "\n",
    "    @property\n",
    "    def index2word(self):\n",
    "        return self.__vocab.get_itos()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.__vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "Vi = Language(train.vi,3,specials,UNK_IDX)\n",
    "En = Language(train.en,3,specials,UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23972"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(En.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17843"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vi.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab.vocab import Vocab\n",
    "def data_preprocessing(data:List[List[str]],vocab:Vocab):\n",
    "    rr = []\n",
    "    idx2word = vocab.get_itos()\n",
    "    for line in data:\n",
    "        tkl = ['<sos>']\n",
    "        for word in line:\n",
    "            tkl.append(idx2word[vocab[word]])\n",
    "        tkl.append('<eos>')\n",
    "        rr.append(tkl)\n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_prep = data_preprocessing(train.en, En.vocab)\n",
    "train_vi_prep = data_preprocessing(train.vi, Vi.vocab)\n",
    "test_en_prep = data_preprocessing(test.en, En.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133317\n",
      "133317\n",
      "1268\n"
     ]
    }
   ],
   "source": [
    "print(len(train_en_prep))\n",
    "print(len(train_vi_prep))\n",
    "print(len(test_en_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133317"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(train_en_prep, train_vi_prep))\n",
    "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
    "test_data = list(zip(test_en_prep, test.en, test.vi))\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def make_batch(data:List[Tuple], batchsize:int):\n",
    "    bb = []\n",
    "    ben = []\n",
    "    bvi = []\n",
    "    for en, vi in data: \n",
    "        ben.append(en)\n",
    "        bvi.append(vi)\n",
    "        if len(ben) >= batchsize:\n",
    "            bb.append((ben, bvi))\n",
    "            ben = []\n",
    "            bvi = []\n",
    "    if len(ben) > 0:\n",
    "        bb.append((ben, bvi))\n",
    "    return bb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_batch(train_data, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_batch(b):\n",
    "    maxlen = max([len(x) for x in b])\n",
    "    for tkl in b:\n",
    "        for i in range(maxlen - len(tkl)):\n",
    "            tkl.append('<pad>')\n",
    "\n",
    "def padding(bb):\n",
    "    for ben, bvi in bb:\n",
    "        padding_batch(ben)\n",
    "        padding_batch(bvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(data:List[List[str]], vocab: Vocab):\n",
    "    return [[vocab[word] for word in word_lst] for word_lst in data]\n",
    "\n",
    "train_data = [(text_pipeline(ben,En.vocab),text_pipeline(bvi,En.vocab)) for ben, bvi in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocablist_x, vocabidx_x, vocablist_y, vocabidx_y):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_embed_x = len(vocablist_x)\n",
    "        self.num_embed_y = len(vocabidx_y)\n",
    "\n",
    "        self.encemb = torch.nn.Embedding(self.num_embed_x, 256, padding_idx = vocabidx_x['<pad>'])\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.enclstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
    "        \n",
    "        self.decemb = torch.nn.Embedding(self.num_embed_x, 256, padding_idx = vocabidx_y['<pad>'])\n",
    "        self.declstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
    "        self.decout = torch.nn.Linear(516, self.num_embed_y)\n",
    "  \n",
    "    def forward(self,x):\n",
    "        x, y = x[0], x[1]\n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "\n",
    "        e_x = self.dropout(self.encemb(x))\n",
    "        \n",
    "        outenc,(hidden,cell) = self.enclstm(e_x)\n",
    "\n",
    "        n_y=y.shape[0]\n",
    "        print('n_y: ',y.shape)\n",
    "        # outputs = torch.zeros(n_y,BATCHSIZE,self.num_embed_x).to(DEVICE)\n",
    "        loss = torch.tensor(0.,dtype=torch.float32).to(DEVICE)\n",
    "        for i in range(n_y-1):\n",
    "            input = y[i]\n",
    "            input = input.unsqueeze(0)\n",
    "            input = self.dropout(self.decemb(input))\n",
    "            outdec, (hidden,cell) = self.declstm(input,(hidden,cell))\n",
    "            output = self.decout(outdec.squeeze(0))\n",
    "            input = y[i+1]\n",
    "            print(input.shape,output.shape)\n",
    "            loss += F.cross_entropy(output, y[i+1])\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self,x,vocablist_y,vocabidx_y):\n",
    "        e_x = self.dropout(self.encemb(x))\n",
    "        outenc,(hidden,cell)=self.enclstm(e_x)\n",
    "        \n",
    "        y = torch.tensor([vocabidx_y['<cls>']]).to(DEVICE)\n",
    "        pred=[]\n",
    "        for i in range(30):\n",
    "            input = y\n",
    "            input = input.unsqueeze(0)\n",
    "            input = self.dropout(self.decemb(input))\n",
    "            outdec,(hidden,cell)= self.declstm(input,(hidden,cell))\n",
    "            output = self.decout(outdec.squeeze(0))  \n",
    "            pred_id = output.squeeze().argmax().item()\n",
    "            if pred_id == vocabidx_y['<eos>']:\n",
    "                break\n",
    "            pred_y = vocablist_y[pred_id]\n",
    "            pred.append(pred_y)\n",
    "            y[0]=pred_id\n",
    "            input=y\n",
    "        return pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LMST():\n",
    "    model = LSTM(En.index2word, En.word2index, Vi.index2word, Vi.word2index).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR) \n",
    "    for epoch in range(EPOCH):\n",
    "        loss = 0\n",
    "        step = 0\n",
    "        for ben, bvi in train_data:\n",
    "            ben = torch.tensor(ben, dtype=torch.int64).transpose(0,1).to(DEVICE) \n",
    "            bvi = torch.tensor(bvi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            batchloss = model((ben, bvi))\n",
    "            batchloss.backward()\n",
    "            optimizer.step() \n",
    "            loss = loss + batchloss.item()\n",
    "            if step % 100 == 0:\n",
    "                print(\"step:\", step, \"batch loss:\", batchloss.item())\n",
    "            step += 1\n",
    "        print(\"epoch\", epoch, \": loss\", loss)\n",
    "    torch.save(model.state_dict(), MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_y:  torch.Size([3, 128])\n",
      "torch.Size([128]) torch.Size([128, 17843])\n",
      "torch.Size([128]) torch.Size([128, 17843])\n",
      "step: 0 batch loss: 19.568538665771484\n",
      "n_y:  torch.Size([5, 128])\n",
      "torch.Size([128]) torch.Size([128, 17843])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 19889 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\19521204\\Jupyter\\NLP\\Machine Translator.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000035?line=0'>1</a>\u001b[0m train_LMST()\n",
      "\u001b[1;32md:\\19521204\\Jupyter\\NLP\\Machine Translator.ipynb Cell 23'\u001b[0m in \u001b[0;36mtrain_LMST\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000034?line=8'>9</a>\u001b[0m bvi \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(bvi, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000034?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000034?line=10'>11</a>\u001b[0m batchloss \u001b[39m=\u001b[39m model((ben, bvi))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000034?line=11'>12</a>\u001b[0m batchloss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000034?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\19521204\\Jupyter\\NLP\\Machine Translator.ipynb Cell 22'\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000033?line=31'>32</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m y[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000033?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape,output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000033?line=33'>34</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(output, y[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/19521204/Jupyter/NLP/Machine%20Translator.ipynb#ch0000033?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 19889 is out of bounds."
     ]
    }
   ],
   "source": [
    "train_LMST()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import contractions\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, url_en, url_vi):\n",
    "        self.__en_data = self.__load_data(url_en,'en')\n",
    "        self.__vi_data = self.__load_data(url_vi,'vi')\n",
    "\n",
    "    def __text_preprocessing(self, text:str, language:str = 'en'):\n",
    "        text = html.unescape(text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "\n",
    "        if language == 'en':\n",
    "            from underthesea import sent_tokenize\n",
    "            text = contractions.fix(text)\n",
    "        else:\n",
    "            from nltk import sent_tokenize\n",
    "\n",
    "        return [sentence for sentence in sent_tokenize(text)]\n",
    "\n",
    "    def __load_data(self, url, language:str):\n",
    "        return [self.__text_preprocessing(line,language) for line in requests.get(url).text.splitlines()]\n",
    "\n",
    "    @property\n",
    "    def vi(self):\n",
    "        return self.__vi_data\n",
    "\n",
    "    @property\n",
    "    def en(self):\n",
    "        return self.__en_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
    "\n",
    "train = DataLoader(url +'train.en',url +'train.vi')\n",
    "test = DataLoader(url + 'tst2013.en',url + 'tst2013.vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Mỗi năm , hơn 15,000 nhà khoa học đến San Francisco để tham dự hội nghị này .']\n",
      "11 ['Mỗi một khoa học gia đều thuộc một nhóm nghiên cứu , và mỗi nhóm đều nghiên cứu rất nhiều đề tài đa dạng .']\n",
      "12 ['Với chúng tôi , tại Cambridge , các đề tài thay đổi từ sự dao động của El Niño , vốn có tác động đến thời tiết và khí hậu , sự đồng hoá thông tin từ vệ tinh , khí thải từ những cánh đồng nhiên liệu sinh học , tình cờ lại là đề tài tôi nghiên cứu .']\n",
      "13 ['Mỗi lĩnh vực nghiên cứu lại chia ra những lĩnh vực nhỏ hơn , và những nghiên cứu sinh có bằng tiến sĩ , như tôi , phải nghiên cứu những đề tài vô cùng cụ thể , cụ thể như chỉ vài quy trình hay vài phân tử .']\n",
      "14 ['Một trong số những phân tử tôi nghiên cứu tên là isoprene .', 'Đây .', 'Nó là một phân tử hữu cơ nhỏ .', 'Có thể các bạn cũng chưa từng nghe tên .']\n",
      "15 ['Trọng lượng của một chiếc kẹp giấy vào khoảng 900 zeta-illion -- 10 mũ 21 -- phân tử isoprene .']\n",
      "16 ['Dù trọng lượng phân tử rất nhỏ , thế nhưng lượng isoprene được thải vào khí quyển hàng năm ngang ngửa với tổng trọng lượng của dân số toàn cầu .']\n",
      "17 ['Đó là một lượng khí thải khổng lồ , bằng tổng trọng lượng của mêtan .']\n",
      "18 ['Chính vì lượng khí thải rất lớn , nó có ý nghĩa quan trọng với hệ thống khí quyển .']\n",
      "19 ['Chính vì nó có ý nghĩa quan trọng với hệ thống khí quyển , giá nào chúng tôi cũng theo đuổi nghiên cứu này đến cùng .']\n",
      "133317\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    print(i,train.vi[i])\n",
    "print(len(train.en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name: str = 'en'):\n",
    "        if name == 'en':\n",
    "            self.__name = 'en'\n",
    "            self.__tokenizer = get_tokenizer('spacy', language ='en_core_web_sm') \n",
    "        else:\n",
    "            self.__name = 'vi'\n",
    "            self.__tokenizer = word_tokenize\n",
    "\n",
    "        self.__vocab = None\n",
    "\n",
    "    def __yield_tokens(self, data):\n",
    "        for line in data:\n",
    "            for sentence in line:\n",
    "                result = self.__tokenizer(sentence)\n",
    "                if result is not None:\n",
    "                    yield result  \n",
    "\n",
    "    def make_vocab(self, train_iter: Iterator, min_freq:int = 1,specials: Optional[List[str]] = None, default_idx:int = 0):\n",
    "        self.__vocab = build_vocab_from_iterator(self.__yield_tokens(train_iter), min_freq, specials)\n",
    "        self.__vocab.set_default_index(default_idx)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.__vocab\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self.__yield_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "Vi = Language('vi')\n",
    "En = Language('en')\n",
    "Vi.make_vocab(train.vi,3,specials,UNK_IDX)\n",
    "En.make_vocab(train.en,3,specials,UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size vi: 18141\n",
      "vocab size en: 23974\n",
      "<unk> 0\n",
      "<pad> 1\n",
      "<sos> 2\n",
      "<eos> 3\n",
      ", 4\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size vi:\", len(Vi.vocab.get_itos()))\n",
    "print(\"vocab size en:\", len(En.vocab.get_itos()))\n",
    "for word in Vi.vocab.get_itos()[:5]:\n",
    "    print(word,Vi.vocab[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(\"it 's\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Over', '15,000', 'scientists', 'go', 'to', 'San', 'Francisco', 'every', 'year', 'for', 'that', '.']\n",
      "['And', 'every', 'one', 'of', 'those', 'scientists', 'is', 'in', 'a', 'research', 'group', ',', 'and', 'every', 'research', 'group', 'studies', 'a', 'wide', 'variety', 'of', 'topics', '.']\n",
      "['For', 'us', 'at', 'Cambridge', ',', 'it', \"'s\", 'as', 'varied', 'as', 'the', 'El', 'Niño', 'oscillation', ',', 'which', 'affects', 'weather', 'and', 'climate', ',', 'to', 'the', 'assimilation', 'of', 'satellite', 'data', ',', 'to', 'emissions', 'from', 'crops', 'that', 'produce', 'biofuels', ',', 'which', 'is', 'what', 'I', 'happen', 'to', 'study', '.']\n",
      "['And', 'in', 'each', 'one', 'of', 'these', 'research', 'areas', ',', 'of', 'which', 'there', 'are', 'even', 'more', ',', 'there', 'are', 'PhD', 'students', ',', 'like', 'me', ',', 'and', 'we', 'study', 'incredibly', 'narrow', 'topics', ',', 'things', 'as', 'narrow', 'as', 'a', 'few', 'processes', 'or', 'a', 'few', 'molecules', '.']\n",
      "['And', 'one', 'of', 'the', 'molecules', 'I', 'study', 'is', 'called', 'isoprene', ',', 'which', 'is', 'here', '.']\n",
      "['It', \"'s\", 'a', 'small', 'organic', 'molecule', '.']\n",
      "['You', \"'\", 've', 'probably', 'never', 'heard', 'of', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "for text in En.tokenizer(train.en[10:15]):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data:List[List], lang: Language):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import contractions\n",
    "import requests\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, url_en, url_vi):\n",
    "        self.__en_data = self.__load_data(url_en,'en')\n",
    "        self.__vi_data = self.__load_data(url_vi,'vi')\n",
    "\n",
    "    def __text_preprocessing(self, text:str, language:str = 'en'):\n",
    "        text = html.unescape(text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        if language == 'en':\n",
    "            from underthesea import sent_tokenize\n",
    "            text = contractions.fix(text)\n",
    "        else:\n",
    "            from nltk import sent_tokenize\n",
    "\n",
    "        return [sentence for sentence in sent_tokenize(text)]\n",
    "\n",
    "    def __load_data(self, url, language:str):\n",
    "        return [self.__text_preprocessing(line,language) for line in requests.get(url).text.splitlines()]\n",
    "\n",
    "    @property\n",
    "    def vi(self):\n",
    "        return self.__vi_data\n",
    "\n",
    "    @property\n",
    "    def en(self):\n",
    "        return self.__en_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
    "\n",
    "train = DataLoader(url +'train.en',url +'train.vi')\n",
    "test = DataLoader(url + 'tst2013.en',url + 'tst2013.vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Khoa học đằng sau một tiêu đề về khí hậu']\n",
      "133317\n"
     ]
    }
   ],
   "source": [
    "print(train.vi[0])\n",
    "print(len(train.en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name: str = 'en'):\n",
    "        if name == 'en':\n",
    "            self.__name = 'en'\n",
    "            self.__tokenizer = get_tokenizer('spacy', language ='en_core_web_sm') \n",
    "        else:\n",
    "            self.__name = 'vi'\n",
    "            self.__tokenizer = word_tokenize\n",
    "\n",
    "        self.__vocab = None\n",
    "\n",
    "    def __yield_tokens(self, data):\n",
    "        for line in data:\n",
    "            for sentence in line:\n",
    "                yield self.__tokenizer(sentence) \n",
    "\n",
    "    def make_vocab(self, train_iter: Iterator, min_freq:int = 1,specials: Optional[List[str]] = None, default_idx:int = 0):\n",
    "        self.__vocab = build_vocab_from_iterator(self.__yield_tokens(train_iter), min_freq, specials)\n",
    "        self.__vocab.set_default_index(default_idx)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.__vocab\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self.__tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "Vi = Language('vi')\n",
    "En = Language('en')\n",
    "Vi.make_vocab(train.vi,3,specials,UNK_IDX)\n",
    "En.make_vocab(train.en,3,specials,UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size vi: 18141\n",
      "vocab size en: 23974\n",
      "<unk> 0\n",
      "<pad> 1\n",
      "<sos> 2\n",
      "<eos> 3\n",
      ", 4\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size vi:\", len(Vi.vocab.get_itos()))\n",
    "print(\"vocab size en:\", len(En.vocab.get_itos()))\n",
    "for word in Vi.vocab.get_itos()[:5]:\n",
    "    print(word,Vi.vocab[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import Vocab\n",
    "\n",
    "def data_preprocessing(train_data : List[str], vocab: Vocab):\n",
    "    prep_data = []\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
